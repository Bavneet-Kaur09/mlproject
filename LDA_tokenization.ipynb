{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XH2rVOcaiIXD"
      },
      "outputs": [],
      "source": [
        "from mlxtend.preprocessing import TransactionEncoder\n",
        "from mlxtend.frequent_patterns import apriori\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import html\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.feature_extraction.text import CountVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load the mental health dataset\n",
        "data = pd.read_csv(\"Mental-Health-Twitter.csv\")"
      ],
      "metadata": {
        "id": "MOAp-8eHicsW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = data[\"post_text\"]\n",
        "y = data[\"label\"]"
      ],
      "metadata": {
        "id": "KjCg7yh-ifiW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kM3yRSntijIk",
        "outputId": "6d4b4612-bfc6-4e9c-d350-f20aeac0c27f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TAG_RE = re.compile(r'<[^>]+>')\n",
        "\n",
        "def remove_tags(text):\n",
        "    '''Removes HTML tags: replaces anything between opening and closing <> with empty space'''\n",
        "\n",
        "    return TAG_RE.sub('', text)"
      ],
      "metadata": {
        "id": "4u1bC4aOiqWk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(sen):\n",
        "    '''Cleans text data up, leaving only 2 or more char long non-stopwords composed of A-Z & a-z only\n",
        "    in lowercase'''\n",
        "\n",
        "\n",
        "    # lowercasing...\n",
        "    sentence = sen.lower()\n",
        "\n",
        "    # Remove RT symbol\n",
        "    sentence = re.sub(r'^(RT|rt)[\\s]+', '', sentence)\n",
        "\n",
        "    # Remove username @ symbol\n",
        "    sentence = re.sub(r'@\\w+', '', sentence)\n",
        "\n",
        "    # Remove the word 'user'\n",
        "    sentence = re.sub(r'\\buser\\b', '', sentence)\n",
        "\n",
        "    # Remove the word 'yong'\n",
        "    sentence = re.sub(r'\\byong\\b', '', sentence)\n",
        "\n",
        "    # Remove the word 'aleph'\n",
        "    sentence = re.sub(r'\\baleph\\b', '', sentence)\n",
        "\n",
        "    # Remove the word 'paytforluckysun'\n",
        "    sentence = re.sub(r'\\bpaytforluckysun\\b', '', sentence)\n",
        "\n",
        "    # Remove the word 'joe'\n",
        "    sentence = re.sub(r'\\bjoe\\b', '', sentence)\n",
        "\n",
        "    # Remove the word 'wearepayting'\n",
        "    sentence = re.sub(r'\\bwearepayting\\b', '', sentence)\n",
        "\n",
        "    # Remove the word 'foryong'\n",
        "    sentence = re.sub(r'\\bforyong\\b', '', sentence)\n",
        "\n",
        "    # Remove the word 'sos'\n",
        "    sentence = re.sub(r'\\bsos\\b', '', sentence)\n",
        "\n",
        "    # Remove the word 'mnwild'\n",
        "    sentence = re.sub(r'\\bmnwild\\b', '', sentence)\n",
        "\n",
        "    # Remove the word 'bbmas'\n",
        "    sentence = re.sub(r'\\bbbmas\\b', '', sentence)\n",
        "\n",
        "    # Remove html tags\n",
        "    sentence = remove_tags(sentence)\n",
        "\n",
        "    # Remove URLs/links\n",
        "    sentence = re.sub(r\"http\\S+\", \"\", sentence)\n",
        "\n",
        "    # Replace html encoded characters\n",
        "    sentence = html.unescape(sentence)\n",
        "\n",
        "    # Remove punctuations and numbers\n",
        "    sentence = re.sub('[^a-zA-Z]', ' ', sentence)\n",
        "\n",
        "    # Single character removal\n",
        "    sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence)  # When we remove apostrophe from the word \"Mark's\", the apostrophe is replaced by an empty space. Hence, we are left with single character \"s\" that we are removing here.\n",
        "\n",
        "    # Remove multiple spaces\n",
        "    sentence = re.sub(r'\\s+', ' ', sentence)  # Next, we remove all the single characters and replace it by a space which creates multiple spaces in our text. Finally, we remove the multiple spaces from our text as well.\n",
        "\n",
        "    # Remove Stopwords\n",
        "    pattern = re.compile(r'\\b(' + r'|'.join(stopwords.words('english')) + r')\\b\\s*')\n",
        "    sentence = pattern.sub('', sentence)\n",
        "\n",
        "    return sentence"
      ],
      "metadata": {
        "id": "xFoW0YSXireb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['preprocessed_text'] = data['post_text'].apply(preprocess_text)"
      ],
      "metadata": {
        "id": "_amyIkcQi087"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "\n",
        "# Filter data for label 0\n",
        "df_label0 = data[data['label'] == 0]\n",
        "\n",
        "# Create count vectorizer\n",
        "vectorizer0 = CountVectorizer()\n",
        "X0 = vectorizer0.fit_transform(df_label0['preprocessed_text'])\n",
        "\n",
        "# Perform LDA\n",
        "lda0 = LatentDirichletAllocation(n_components=5, random_state=0)\n",
        "lda0.fit(X0)\n",
        "\n",
        "# Get topics and print top words for each topic\n",
        "feature_names0 = vectorizer0.get_feature_names_out()\n",
        "for topic_idx, topic in enumerate(lda0.components_):\n",
        "    print(\"Topic %d:\" % (topic_idx))\n",
        "    print([feature_names0[i] for i in topic.argsort()[:-10 - 1:-1]])\n",
        "\n",
        "\n",
        "# Filter data for label 1\n",
        "df_label1 = data[data['label'] == 1]\n",
        "\n",
        "# Create count vectorizer\n",
        "vectorizer1 = CountVectorizer()\n",
        "X1 = vectorizer1.fit_transform(df_label1['preprocessed_text'])\n",
        "\n",
        "# Perform LDA\n",
        "lda1 = LatentDirichletAllocation(n_components=5, random_state=0)\n",
        "lda1.fit(X1)\n",
        "\n",
        "# Get topics and print top words for each topic\n",
        "feature_names1 = vectorizer1.get_feature_names_out()\n",
        "for topic_idx, topic in enumerate(lda1.components_):\n",
        "    print(\"Topic %d:\" % (topic_idx))\n",
        "    print([feature_names1[i] for i in topic.argsort()[:-10 - 1:-1]])\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oqojbV_FZV13",
        "outputId": "ec8e1284-7317-4c01-c361-ed9e02a7cb73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topic 0:\n",
            "['like', 'get', 'time', 'trump', 'look', 'one', 'know', 'christmas', 'really', 'think']\n",
            "Topic 1:\n",
            "['hey', 'thanks', 'oh', 'best', 'follow', 'zayn', 'bestmusicvideo', 'pillowtalk', 'iheartawards', 'never']\n",
            "Topic 2:\n",
            "['trump', 'like', 'putin', 'one', 'going', 'us', 'hate', 'via', 'america', 'russia']\n",
            "Topic 3:\n",
            "['thank', 'say', 'twitter', 'love', 'following', 'hello', 'anytime', 'fuck', 'know', 'real']\n",
            "Topic 4:\n",
            "['new', 'people', 'year', 'see', 'need', 'game', 'one', 'still', 'go', 'lightsaber']\n",
            "Topic 0:\n",
            "['depression', 'treatments', 'overcome', 'anxiety', 'health', 'treatment', 'therapy', 'new', 'like', 'help']\n",
            "Topic 1:\n",
            "['love', 'good', 'talk', 'like', 'day', 'today', 'get', 'one', 'want', 'see']\n",
            "Topic 2:\n",
            "['people', 'positive', 'get', 'via', 'thinking', 'life', 'headache', 'like', 'migraine', 'negative']\n",
            "Topic 3:\n",
            "['go', 'one', 'autism', 'please', 'us', 'know', 'love', 'new', 'make', 'need']\n",
            "Topic 4:\n",
            "['like', 'know', 'get', 'shit', 'right', 'im', 'still', 'people', 'feel', 'pain']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create count vectorizer\n",
        "vectorizer_all = CountVectorizer()\n",
        "X_all = vectorizer_all.fit_transform(data['preprocessed_text'])\n",
        "\n",
        "# Perform LDA\n",
        "lda_all = LatentDirichletAllocation(n_components=10, random_state=0)\n",
        "lda_all.fit(X_all)\n",
        "\n",
        "# Get topics and print top words for each topic\n",
        "feature_names_all = vectorizer_all.get_feature_names_out()\n",
        "for topic_idx, topic in enumerate(lda_all.components_):\n",
        "    print(\"Topic %d:\" % (topic_idx))\n",
        "    print([feature_names_all[i] for i in topic.argsort()[:-10 - 1:-1]])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qhjnJRosdZtG",
        "outputId": "c92d222e-64cf-4400-c477-83a32e397e43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topic 0:\n",
            "['new', 'think', 'would', 'year', 'better', 'like', 'never', 'one', 'people', 'even']\n",
            "Topic 1:\n",
            "['please', 'go', 'trump', 'people', 'nanny', 'anxiety', 'asks', 'migraine', 'team', 'via']\n",
            "Topic 2:\n",
            "['depression', 'treatments', 'fuck', 'overcome', 'health', 'mental', 'like', 'treatment', 'may', 'one']\n",
            "Topic 3:\n",
            "['trump', 'putin', 'man', 'via', 'russia', 'oh', 'like', 'life', 'let', 'get']\n",
            "Topic 4:\n",
            "['thanks', 'hey', 'follow', 'see', 'happy', 'could', 'wait', 'want', 'yes', 'wish']\n",
            "Topic 5:\n",
            "['like', 'make', 'watch', 'live', 'family', 'miss', 'one', 'even', 'reason', 'first']\n",
            "Topic 6:\n",
            "['thank', 'say', 'twitter', 'following', 'hello', 'anytime', 'one', 'like', 'take', 'real']\n",
            "Topic 7:\n",
            "['people', 'im', 'talk', 'someone', 'like', 'really', 'get', 'one', 'hate', 'shit']\n",
            "Topic 8:\n",
            "['good', 'get', 'best', 'video', 'bestmusicvideo', 'pillowtalk', 'iheartawards', 'zayn', 'well', 'still']\n",
            "Topic 9:\n",
            "['know', 'love', 'like', 'much', 'one', 'god', 'need', 'friends', 'time', 'work']\n"
          ]
        }
      ]
    }
  ]
}